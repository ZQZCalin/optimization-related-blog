{"version":3,"file":"static/js/62.85e41b6c.chunk.js","mappings":"wKAKMA,GACJ,UAAC,IAAD,YACA,mDADA,2EAGA,2BACC,iNAKA,uKAIA,mJAMD,mBAEA,SAAC,IAAD,CAASC,MAAM,gBAAf,uBArBA,ydAgCA,SAAC,IAAD,CAASC,KAAK,QAAQD,MAAM,QAA5B,qZASA,SAAC,IAAD,iyCAsBA,SAAC,IAAD,CAASC,KAAK,UAAUD,MAAM,QAA9B,oxBA/DA,ieA8EA,mBAEA,SAAC,IAAD,CAASA,MAAM,QAAf,qCAhFA,yQA0FA,SAAC,IAAD,CAASC,KAAK,QAAQD,MAAM,QAA5B,8QAOA,SAAC,IAAD,ydAcF","sources":["reviews/cutkosky20momentum.js"],"sourcesContent":["import { MathJax } from \"better-react-mathjax\";\r\nimport Proof from \"components/Proof\";\r\nimport { Section } from \"components/Section\";\r\nimport Theorem from \"components/Theorem\";\r\n\r\nconst content = (\r\n  <MathJax>\r\n\t\t<h3>History and Innovation</h3>\r\n\t\tThis paper improves prior results and have the following contributions:\r\n\t\t<ol>\r\n\t\t\t<li>\r\n\t\t\t\tWith momentum, NSGD no longer requires large batch size / small variance. \r\n\t\t\t\tInstead, one stochastic gradient per round is sufficient for {`$O(T^{-1/4})$`} \r\n\t\t\t\trate in the base case.\r\n\t\t\t</li>\r\n\t\t\t<li>\r\n\t\t\t\tWith second-order-smooth assumption, this paper proves improved rate of {`$O(T^{-2/7})$`} \r\n\t\t\t\tfor general loss besides linear regression.\r\n\t\t\t</li>\r\n\t\t\t<li>\r\n\t\t\t\tThis paper also provides adaptive learning rate schedule similar to AdaGrad \r\n\t\t\t\tthat adapts to variance automatically.\r\n\t\t\t</li>\r\n\t\t</ol>\r\n\r\n\t\t<hr/>\r\n\r\n\t\t<Section label=\"sec:base-case\">Base Case</Section>\r\n\r\n\t\tIn the base case, this paper proves that NSGD with momentum achieves {`$O(1/T^{1/4})$`} convergence rate \r\n\t\twithout requiring large batch gradients. The algorithm updates as follows:\r\n\t\t{`\\\\begin{equation*}\r\n\t\t\t\tm_t = \\\\beta_t m_{t-1} + (1-\\\\beta_t) g_t, \\\\quad\r\n\t\t\t\tw_{t+1} = w_t - \\\\eta_t \\\\frac{m_t}{\\\\|m_t\\\\|},\r\n\t\t\\\\end{equation*}`}\r\n\t\twhere {`$g_t=\\\\nabla\\\\ell(w_t,z_t)$ and $m_0=g_1$`}. The proof sketch is as follows. \r\n\t\tWe first prove the one-step inequality:\r\n\t\r\n\t\t<Theorem type=\"Lemma\" label=\"lem:1\">\r\n\t\t\t{`Suppose loss $\\\\L$ is $H$-smooth, then for any sequence $g_t$ and $w_t$ such that \r\n\t\t\t$w_{t+1}=w_t-\\\\eta_t\\\\frac{g_t}{\\\\|g_t\\\\|}$, and define $\\\\epsilon_t = g_t-\\\\nabla\\\\L(w_t)$,\r\n\t\t\t\\\\begin{equation*}\r\n\t\t\t\t\\\\L(w_{t+1}) - \\\\L(w_t) \\\\leq -\\\\frac{\\\\eta_t}{3}\\\\|\\\\nabla\\\\L(w_t)\\\\| + \r\n\t\t\t\t\t\t\\\\frac{8\\\\eta_t}{3}\\\\|\\\\epsilon_t\\\\| + \\\\frac{H\\\\eta_t^2}{2}.\r\n\t\t\t\\\\end{equation*}`}\r\n\t\t</Theorem>\r\n\r\n\t\t<Proof>\r\n\t\t\t{`By smoothness,\r\n\t\t\t\\\\begin{align*}\r\n\t\t\t\t\t\\\\L(w_{t+1}) - \\\\L(w_t)\r\n\t\t\t\t\t&\\\\le \\\\frac{-\\\\eta_t\\\\langle \\\\nabla\\\\L(w_t), g_t\\\\rangle}{\\\\|g_t\\|} + \\\\frac{H\\\\eta_t^2}{2} \\\\\\\\\r\n\t\t\t\t\t&= \\\\frac{-\\\\eta_t\\\\langle \\\\nabla\\\\L(w_t), \\\\nabla\\\\L(w_t)+\\\\epsilon_t\\\\rangle}{\\\\|\\\\nabla\\\\L(w_t)+\\\\epsilon_t\\\\|} + \\\\frac{H\\\\eta_t^2}{2}.\r\n\t\t\t\\\\end{align*}\r\n\t\t\tConsider the following two cases: (1) If $\\\\|\\\\nabla\\\\L(w_t)\\\\| \\\\geq 2\\\\|\\\\epsilon_t\\\\|$,\r\n\t\t\t\\\\begin{align*}\r\n\t\t\t\t\t\\\\frac{-\\\\eta_t\\\\langle \\\\nabla\\\\L(w_t), \\\\nabla\\\\L(w_t)+\\\\epsilon_t\\\\rangle}{\\\\|\\\\nabla\\\\L(w_t)+\\\\epsilon_t\\\\|}\r\n\t\t\t\t\t&\\\\le \\\\frac{-\\\\eta_t\\\\|\\\\nabla\\\\L(w_t)\\\\|^2 + \\\\eta_t\\\\|\\\\nabla\\\\L(w_t)\\\\|\\\\|\\\\epsilon_t\\\\|}{\\\\|\\\\nabla\\\\L(w_t)\\\\epsilon_t\\\\|} \\\\\\\\\r\n\t\t\t\t\t&\\\\le \\\\frac{-\\\\frac{1}{2}\\\\eta_t\\\\|\\\\nabla\\\\L(w_t)\\\\|^2}{\\\\|\\\\nabla\\\\L(w_t)\\\\|+\\\\|\\\\epsilon_t\\\\|} \\\\le -\\\\frac{\\\\eta_t}{3} \\\\|\\\\nabla\\\\L(w_t)\\\\|.\r\n\t\t\t\\\\end{align*}\r\n\t\t\t(2) If $\\\\|\\\\nabla\\\\L(w_t)\\\\|\\\\leq 2\\\\|\\\\epsilon_t\\\\|$,\r\n\t\t\t\\\\begin{align*}\r\n\t\t\t\t\t\\\\frac{-\\\\eta_t\\\\langle \\\\nabla\\\\L(w_t), g_t\\\\rangle}{\\\\|g_t\\\\|}\r\n\t\t\t\t\t&\\\\le \\\\eta_t\\\\|\\\\nabla\\\\L(w_t)\\\\|\r\n\t\t\t\t\t\\\\le -\\\\frac{\\\\eta_t}{3}\\\\|\\\\nabla\\\\L(w_t)\\\\| + \\\\frac{8\\\\eta_t}{3}\\\\|\\\\epsilon_t\\\\|.\r\n\t\t\t\\\\end{align*}\r\n\t\t\tCombining two cases gives the inequality.`}\r\n\t\t</Proof>\r\n\r\n\t\t<Theorem type=\"Theorem\" label=\"thm:1\">\r\n\t\t\t{`Fix $\\\\eta_t=\\\\eta$ and $\\\\beta_t=1-\\\\alpha$, and assume $\\\\L(w_1)-\\\\inf\\\\L\\\\leq \\\\Delta$ and $\\\\E[\\\\|g_t-\\\\nabla\\\\L(w_t)\\\\|^2]\\\\le \\\\sigma^2$. Then\r\n\t\t\t\\\\begin{equation*}\r\n\t\t\t\t\t\\\\sum_{t=1}^T\\\\E[\\\\|\\\\nabla\\\\L(w_t)\\\\|] \\\\leq O\\\\left(\\\\frac{\\\\Delta}{\\\\eta} + \\\\frac{\\\\sigma+H\\\\eta T}{\\\\alpha} + \\\\sigma\\\\sqrt{\\\\alpha} T\\\\right). \r\n\t\t\t\\\\end{equation*}\r\n\t\t\tMoreover, if we choose $\\\\eta=\\\\sqrt{\\\\frac{\\\\Delta\\\\alpha}{HT}}$ and $\\\\alpha=\\\\min(1, \\\\frac{\\\\sqrt{\\\\Delta H}}{\\\\sigma\\\\sqrt{T}})$, then\r\n\t\t\t\\\\begin{equation*}\r\n\t\t\t\t\t\\\\frac{1}{T}\\\\sum_{t=1}^T\\\\E[\\\\|\\\\nabla\\\\L(w_t)\\\\|] \\\\leq O\\\\left( \\\\frac{\\\\sqrt{\\\\Delta H}}{\\\\sqrt{T}} + \\\\frac{\\\\sqrt{\\\\sigma\\\\sqrt{\\\\Delta H}}}{T^{1/4}} + \\\\frac{\\\\sigma^2}{\\\\sqrt{\\\\Delta HT}} \\\\right). \r\n\t\t\t\\\\end{equation*}`}\r\n\t\t</Theorem>\r\n\r\n\t\t{`As a remark, in the proof of Theorem \\\\ref{thm:base}, it's proved that $\\\\E[\\\\|m_t-\\\\nabla\\\\L(w_t)\\\\|] \\\\leq (1-\\\\alpha)^{t-1}\\\\sigma + \\\\sqrt{\\\\alpha}\\\\sigma +\\\\frac{H\\\\eta}{\\\\alpha}$, \r\n\t\twhich is $O((1-\\\\alpha)^{t-1}\\\\sigma + \\\\frac{\\\\sqrt{\\\\sigma}}{T^{1/4}}+\\\\frac{1}{\\\\sqrt{T}})$ under the proper choice of $\\\\eta$ and $\\\\alpha$. Compared to the standard SGD with \r\n\t\t$\\\\E[\\\\|g_t-\\\\nabla\\\\L(w_t)\\\\|] \\\\leq \\\\sigma$, NSGD with momentum effectively reduces the variance.`}\r\n\t\r\n\t\t<hr/>\r\n\r\n\t\t<Section label=\"sec:2\">Second-Order Smoothness</Section>\r\n\r\n\t\t{`\r\n\t\tWe say a function $\\\\L:\\\\RR^d\\\\to\\\\L$ is $\\\\rho$-second-order-smooth if\r\n\t\t\\\\begin{equation}\r\n\t\t\\\\|\\\\nabla^2\\\\L(x)-\\\\nabla^2\\\\L(y)\\\\|_{\\\\mathrm{op}} \\\\leq \\\\rho\\\\|x-y\\\\|, \\\\ \\\\forall x,y.\r\n\t\t\\\\label{eq:second-order-smooth}\r\n\t\t\\\\end{equation}\r\n\t\t`}\r\n\r\n\t\t<Theorem type=\"Lemma\" label=\"lem:2\">\r\n\t\t\t{`If $\\\\L:\\\\RR^d\\\\to\\\\L$ is $\\\\rho$-second-order-smooth \\\\eqref{eq:second-order-smooth}, then\r\n\t\t\t\\\\begin{equation*}\r\n\t\t\t\t\t\\\\|Z(x,y)\\\\| \\\\leq \\\\rho\\\\|x-y\\\\|^2, \\\\quad Z(x,y) = \\\\nabla\\\\L(x)-\\\\nabla\\\\L(y)-\\\\nabla^2\\\\L(y)(x-y).\r\n\t\t\t\\\\end{equation*}`}\r\n\t\t</Theorem>\r\n\r\n\t\t<Proof>\r\n\t\t\t{`Define $f:[0,1]\\\\to\\\\RR^d$ by $f(t) = \\\\nabla\\\\L(y+t(x-y)) - \\\\nabla^2\\\\L(y)(y+t(x-y))$. Then by the mean value theorem, for some $t\\\\in[0,1]$ and $z=y+t(x-y)$,\r\n\t\t\t\\\\begin{align*}\r\n\t\t\t\t\t\\\\|Z(x,y)\\\\| = \\\\|f(1) - f(0)\\\\|\r\n\t\t\t\t\t&\\\\le \\\\|f'(t)\\\\| \\\\\\\\\r\n\t\t\t\t\t&= \\\\|\\\\nabla^2\\\\L(z)(x-y) - \\\\nabla^2\\\\L(y)(x-y)\\\\| \\\\\\\\\r\n\t\t\t\t\t&\\\\leq \\\\rho\\\\|z-y\\\\|\\\\|x-y\\\\| \\\\\\\\\r\n\t\t\t\t\t&= \\\\rho t\\\\|x-y\\\\|^2 \\\\leq \\\\rho\\\\|x-y\\\\|^2.\r\n\t\t\t\\\\end{align*}`}\r\n\t\t</Proof>\r\n\r\n\t</MathJax>\r\n);\r\n\r\nexport default content;"],"names":["content","label","type"],"sourceRoot":""}